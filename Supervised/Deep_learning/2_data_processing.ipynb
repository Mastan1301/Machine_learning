{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "546173e3-d952-4329-91c8-a888c0b89a56",
   "metadata": {},
   "source": [
    "# Data processing for Sentimental analysis\n",
    "We create a lexicon $L$ of words. Let $w$ be the no. of words in $L$. For eg.: Let $L$ = chair, table, spoon, television] and we have a new sentence *\"I pulled the chair up to the table.*. We represent this sentence using a hot array $C$ of size $w = 4$ where \n",
    "$$C[i] = \\text{count of } L[i] \\text{ in the sentence}$$\n",
    "\n",
    "So, the new sentence is represented as $[1, 1, 0, 0]$.\n",
    "\n",
    "Stemming -> Ignores the morphological variant of the words and reduces them to the root word.\n",
    "Lemmatizing -> Groups together the different words of the same context, even though they don't have the same root word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae7ccf10-cb49-44ca-9686-86759f052ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ff743e-b404-4631-8526-5ad69be20581",
   "metadata": {},
   "source": [
    "## Creating the dataset from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f23014e-5f49-4ed4-b17a-8282cc0bce53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "n_lines = 10000000\n",
    "pos, neg = \"../data/pos.txt\", \"../data/neg.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ecdafe0b-220e-4373-8ddd-a8a72c5db53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lexicon(pos, neg):\n",
    "    lexicon = []\n",
    "    for file in [pos, neg]:\n",
    "        with open(file, 'r') as f:\n",
    "            contents = f.readlines()\n",
    "            for l in contents[:n_lines]:\n",
    "                all_words = word_tokenize(l)\n",
    "                lexicon += list(all_words)\n",
    "                \n",
    "    lexicon = [lemmatizer.lemmatize(i.lower()) for i in lexicon]\n",
    "    word_counts = Counter(lexicon)\n",
    "    l2 = []\n",
    "    for w in word_counts:\n",
    "        if 1000 > word_counts[w] > 50:\n",
    "            l2.append(w)\n",
    "            \n",
    "    return l2\n",
    "\n",
    "def represent_sentence(sentence, lexicon):\n",
    "    result = np.zeros(len(lexicon))\n",
    "    \n",
    "    current_words = word_tokenize(sentence.lower())\n",
    "    current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
    "    for word in current_words:\n",
    "        if word.lower() in lexicon:\n",
    "            index_value = lexicon.index(word.lower())\n",
    "            result[index_value] += 1\n",
    "\n",
    "    result = list(result)\n",
    "            \n",
    "    return result\n",
    "                \n",
    "def sample_handling(sample, lexicon, classification):\n",
    "    featureset = []\n",
    "    \n",
    "    with open(sample, 'r') as f:\n",
    "        contents = f.readlines()\n",
    "        for l in contents[:n_lines]:\n",
    "            features = represent_sentence(l, lexicon)\n",
    "            featureset.append([features, classification])\n",
    "            \n",
    "    return featureset\n",
    "\n",
    "def create_feature_sets_and_labels(pos, neg, lexicon, test_size_ratio=0.1):\n",
    "    features = []\n",
    "    features += sample_handling(pos, lexicon, [1, 0])\n",
    "    features += sample_handling(neg, lexicon, [0, 1])\n",
    "    random.shuffle(features)\n",
    "    \n",
    "    features = np.array(features, dtype=object)\n",
    "    testing_size = int(test_size_ratio * len(features))\n",
    "    \n",
    "    train_x = list(features[:, 0][:-testing_size])\n",
    "    train_y = list(features[:, 1][:-testing_size])\n",
    "    train_x = np.array([np.array(i, dtype=np.float64) for i in train_x])\n",
    "    train_y = np.array([np.array(i, dtype=np.float64) for i in train_y])\n",
    "    \n",
    "    test_x = list(features[:, 0][-testing_size:])\n",
    "    test_y = list(features[:, 1][-testing_size:])\n",
    "    test_x = np.array([np.array(i, dtype=np.float64) for i in test_x])\n",
    "    test_y = np.array([np.array(i, dtype=np.float64) for i in test_y])\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c975c95c-d848-46a8-99b7-f7437604e203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     train_x, train_y, test_x, test_y = create_feature_sets_and_labels(pos, neg, lexicon)\n",
    "#     with open('../data/sentiment_set.pickle', 'wb') as f:\n",
    "#         pickle.dump([train_x, train_y, test_x, test_y], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5803845a-afd7-427c-a4b1-c120a9419fdc",
   "metadata": {},
   "source": [
    "## Applying the neural network on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bea12f-cbe4-43a2-b169-351a1982a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "636c3744-b268-4315-89fd-cbbf8bbda600",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes_hl1 = 50\n",
    "n_nodes_hl2 = 20\n",
    "n_nodes_hl3 = 10\n",
    "\n",
    "n_classes = 2\n",
    "batch_size = int(n_lines / 50.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4efa0e0f-85a8-4c5d-914b-91ce1b75fb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = create_lexicon(pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d15973a0-179f-4497-bebe-904cf979beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = create_feature_sets_and_labels(pos, neg, lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2dbd008a-d5f0-4cc5-8b1c-7eb7023c6447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1/1 [==============================] - 0s 482ms/step - loss: 0.6964 - categorical_accuracy: 0.4930 - val_loss: 0.6940 - val_categorical_accuracy: 0.5156\n",
      "Epoch 2/6\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.6937 - categorical_accuracy: 0.5079 - val_loss: 0.6926 - val_categorical_accuracy: 0.5214\n",
      "Epoch 3/6\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6914 - categorical_accuracy: 0.5234 - val_loss: 0.6913 - val_categorical_accuracy: 0.5260\n",
      "Epoch 4/6\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.6893 - categorical_accuracy: 0.5423 - val_loss: 0.6901 - val_categorical_accuracy: 0.5286\n",
      "Epoch 5/6\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6873 - categorical_accuracy: 0.5612 - val_loss: 0.6889 - val_categorical_accuracy: 0.5354\n",
      "Epoch 6/6\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6855 - categorical_accuracy: 0.5746 - val_loss: 0.6878 - val_categorical_accuracy: 0.5495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ad9e23d4b0>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(len(train_x[0]),)),\n",
    "    tf.keras.layers.Dense(n_nodes_hl1, activation='relu'),\n",
    "    tf.keras.layers.Dense(n_nodes_hl2, activation='relu'),\n",
    "    tf.keras.layers.Dense(n_classes),\n",
    "]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x=train_x, \n",
    "    y=train_y,\n",
    "    epochs=6,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2910d8-0b6d-4747-91ab-cc99a228e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = model.evaluate(test_x, test_y, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c9a5836d-f683-48c7-a417-2c633f54109a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.02442591 -0.02480735]\n",
      " [-0.18129416  0.06998322]\n",
      " [-0.09880796  0.08143519]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "unknown = tf.convert_to_tensor(represent_sentence(\"This is wrong\", lexicon))\n",
    "print(model(test_x[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3118f2d7-dc87-43f1-ba8c-1ae37623ada3",
   "metadata": {},
   "source": [
    "Since, the dataset is very small, the accuracy is not good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67782310-ca3f-416a-9f5d-5d30d51f531e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
