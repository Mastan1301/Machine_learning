{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91d37ac3-aa47-4fd1-9278-c8b01fab2d56",
   "metadata": {},
   "source": [
    "# Deep Q-learning \n",
    "Also called Deep Q networks (DNQs). Deep learning versions of Q-learning. \n",
    "\n",
    "* With DQNs, instead of a Q Table to look up values, you have a model that you inference (make predictions from), and rather than updating the Q table, you fit (train) your model.\n",
    "\n",
    "It is a regression model, which typically will output values for each of our possible actions. These values will be continuous float values, and they are directly our Q values.\n",
    "\n",
    "The complexity is higher, but the memory required is not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13a0a5aa-d1a9-460c-990e-0a1590f4ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04e02b0f-917c-4d1a-9983-97e11cb26985",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT = 0.99\n",
    "REPLAY_MEMORY_SIZE = 50000\n",
    "MIN_REPLAY_MEMORY_SIZE = 1000\n",
    "MINIBATCH_SIZE = 64\n",
    "UPDATE_TARGET_EVERY = 5\n",
    "MODEL_NAME = '2X256'\n",
    "MIN_REWARD = -200\n",
    "MEMORY_FRACTION = 0.2\n",
    "\n",
    "EPISODES = 20000\n",
    "epsilon = 1\n",
    "EPSILON_DECAY = 0.99975\n",
    "MIN_EPSILON = 0.001\n",
    "\n",
    "AGGREGATE_STATS_EVERY = 50\n",
    "SHOW_PREVIEW = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6192ecd-e1a6-48f4-9b8c-3b6e9f6bd8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        # Main model\n",
    "        self.model = self.create_model()\n",
    "        \n",
    "        #Target network\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        # An array with last n steps for training\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "        \n",
    "        # Custom tensorboard object\n",
    "        self.tensorboard = ModifiedTensorBoard(log_dir=f\"logs/{MODEL_NAME}-{int(time.time())}\")\n",
    "        \n",
    "        # Used to count when to update target network with main network's weights\n",
    "        self.target_update_counter = 0        \n",
    "        \n",
    "    def create_model(self):\n",
    "        model = tf.keras.models.Sequential([\n",
    "                    tf.keras.layers.Conv2D(50, (3, 3), input_shape=env.OBSERVATION_SPACE_VALUES, activation='relu'),\n",
    "                    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                    tf.keras.layers.Dropout(0.2),\n",
    "                \n",
    "                    tf.keras.layers.Conv2D(20, (3, 3), input_shape=env.OBSERVATION_SPACE_VALUES, activation='relu'),\n",
    "                    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                    tf.keras.layers.Dropout(0.2),\n",
    "                    \n",
    "                    tf.keras.layers.Flatten(),\n",
    "                    tf.keras.layers.Dense(15),\n",
    "            \n",
    "                    tf.keras.layers.Dense(env.ACTION_SPACE_SIZE, activation='linear'),\n",
    "                ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "            loss='mean_absolute_error',\n",
    "            metrics=['accuracy',],\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    # Adds step's data to a memory replay array\n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "        \n",
    "    # Queries the main network for Q values given current state\n",
    "    def get_qs(self, state):\n",
    "        return self.model.predict(np.array(state).reshape(-1, *state.shape) / 255)[0]\n",
    "    \n",
    "    def train(self, terminal_state, step):\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "        \n",
    "        # Get a mini-batch of random samples from the memory replay table\n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "        \n",
    "        # Get current states from minibatch, then query NN model for Q values\n",
    "        current_states = np.array([transition[0] for transition in minibatch]) / 255\n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "        \n",
    "        # Get future states from minibatch, then query NN model for Q values\n",
    "        # When using target network, query it, otherwise main network should be queried\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch]) / 255\n",
    "        futures_qs_list = self.target_model.predict(new_current_states)\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "                \n",
    "            else:\n",
    "                new_q = reward\n",
    "                \n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "            \n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "            \n",
    "        self.model.fit(\n",
    "            np.array(X) / 255, \n",
    "            np.array(y), \n",
    "            batch_size=MINIBATCH_SIZE, \n",
    "            verbose=0, \n",
    "            shuffle=False, \n",
    "            callbacks=[self.tensorboard] if terminal_state else None)      \n",
    "        \n",
    "        # Update target network counter every episode\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "        \n",
    "        # If counter reaches set value, update target network with weights of main network\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1429a07-9f94-4fee-903d-43fb88ecd573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Own Tensorboard class\n",
    "class ModifiedTensorBoard(TensorBoard):\n",
    "    # Overriding init to set initial step and writer\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir)\n",
    "        \n",
    "    # Stop creating default log writer\n",
    "    def set_model(self, model):\n",
    "        pass\n",
    "    \n",
    "    # Saves logs with our step no.\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "        \n",
    "    # We train for one batch only, no need to save anything at epoch end\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "    \n",
    "    # Won't close writer\n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "    \n",
    "    # Custom method for saving own metrics\n",
    "    # Creates writer, writes custom metrics and closes writer\n",
    "    def update_stats(self, **stats):\n",
    "        self._write_logs(stats, self.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f10cfdf8-58aa-4483-baf7-ba128a02535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Blob:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.x = np.random.randint(0, size)\n",
    "        self.y = np.random.randint(0, size)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Blob ({self.x}, {self.y})\"\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return (self.x-other.x, self.y-other.y)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "\n",
    "    def action(self, choice):\n",
    "        '''\n",
    "        Gives us 9 total movement options. (0,1,2,3,4,5,6,7,8)\n",
    "        '''\n",
    "        if choice == 0:\n",
    "            self.move(x=1, y=1)\n",
    "        elif choice == 1:\n",
    "            self.move(x=-1, y=-1)\n",
    "        elif choice == 2:\n",
    "            self.move(x=-1, y=1)\n",
    "        elif choice == 3:\n",
    "            self.move(x=1, y=-1)\n",
    "\n",
    "        elif choice == 4:\n",
    "            self.move(x=1, y=0)\n",
    "        elif choice == 5:\n",
    "            self.move(x=-1, y=0)\n",
    "\n",
    "        elif choice == 6:\n",
    "            self.move(x=0, y=1)\n",
    "        elif choice == 7:\n",
    "            self.move(x=0, y=-1)\n",
    "\n",
    "        elif choice == 8:\n",
    "            self.move(x=0, y=0)\n",
    "\n",
    "    def move(self, x=False, y=False):\n",
    "\n",
    "        # If no value for x, move randomly\n",
    "        if not x:\n",
    "            self.x += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.x += x\n",
    "\n",
    "        # If no value for y, move randomly\n",
    "        if not y:\n",
    "            self.y += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.y += y\n",
    "\n",
    "        # If we are out of bounds, fix!\n",
    "        if self.x < 0:\n",
    "            self.x = 0\n",
    "        elif self.x > self.size-1:\n",
    "            self.x = self.size-1\n",
    "        if self.y < 0:\n",
    "            self.y = 0\n",
    "        elif self.y > self.size-1:\n",
    "            self.y = self.size-1\n",
    "\n",
    "class BlobEnv:\n",
    "    SIZE = 10\n",
    "    RETURN_IMAGES = True\n",
    "    MOVE_PENALTY = 1\n",
    "    ENEMY_PENALTY = 300\n",
    "    FOOD_REWARD = 25\n",
    "    OBSERVATION_SPACE_VALUES = (SIZE, SIZE, 3)  # 4\n",
    "    ACTION_SPACE_SIZE = 9\n",
    "    PLAYER_N = 1  # player key in dict\n",
    "    FOOD_N = 2  # food key in dict\n",
    "    ENEMY_N = 3  # enemy key in dict\n",
    "    # the dict! (colors)\n",
    "    d = {1: (255, 175, 0),\n",
    "         2: (0, 255, 0),\n",
    "         3: (0, 0, 255)}\n",
    "\n",
    "    def reset(self):\n",
    "        self.player = Blob(self.SIZE)\n",
    "        self.food = Blob(self.SIZE)\n",
    "        while self.food == self.player:\n",
    "            self.food = Blob(self.SIZE)\n",
    "        self.enemy = Blob(self.SIZE)\n",
    "        while self.enemy == self.player or self.enemy == self.food:\n",
    "            self.enemy = Blob(self.SIZE)\n",
    "\n",
    "        self.episode_step = 0\n",
    "\n",
    "        if self.RETURN_IMAGES:\n",
    "            observation = np.array(self.get_image())\n",
    "        else:\n",
    "            observation = (self.player-self.food) + (self.player-self.enemy)\n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        self.episode_step += 1\n",
    "        self.player.action(action)\n",
    "\n",
    "        #### MAYBE ###\n",
    "        #enemy.move()\n",
    "        #food.move()\n",
    "        ##############\n",
    "\n",
    "        if self.RETURN_IMAGES:\n",
    "            new_observation = np.array(self.get_image())\n",
    "        else:\n",
    "            new_observation = (self.player-self.food) + (self.player-self.enemy)\n",
    "\n",
    "        if self.player == self.enemy:\n",
    "            reward = -self.ENEMY_PENALTY\n",
    "        elif self.player == self.food:\n",
    "            reward = self.FOOD_REWARD\n",
    "        else:\n",
    "            reward = -self.MOVE_PENALTY\n",
    "\n",
    "        done = False\n",
    "        if reward == self.FOOD_REWARD or reward == -self.ENEMY_PENALTY or self.episode_step >= 200:\n",
    "            done = True\n",
    "\n",
    "        return new_observation, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        img = self.get_image()\n",
    "        img = img.resize((300, 300))  # resizing so we can see our agent in all its glory.\n",
    "        cv2.imshow(\"image\", np.array(img))  # show it!\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    # FOR CNN #\n",
    "    def get_image(self):\n",
    "        env = np.zeros((self.SIZE, self.SIZE, 3), dtype=np.uint8)  # starts an rbg of our size\n",
    "        env[self.food.x][self.food.y] = self.d[self.FOOD_N]  # sets the food location tile to green color\n",
    "        env[self.enemy.x][self.enemy.y] = self.d[self.ENEMY_N]  # sets the enemy location to red\n",
    "        env[self.player.x][self.player.y] = self.d[self.PLAYER_N]  # sets the player tile to blue\n",
    "        img = Image.fromarray(env, 'RGB')  # reading to rgb. Apparently. Even tho color definitions are bgr. ???\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "114f3f15-c033-4584-959c-7e7e1204c558",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m BlobEnv()\n\u001b[0;32m      3\u001b[0m ep_rewards \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m200\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m \u001b[43mrandom\u001b[49m\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m tf\u001b[38;5;241m.\u001b[39mset_random_seed(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "env = BlobEnv()\n",
    "\n",
    "ep_rewards = [-200]\n",
    "np.random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "if not os.path.isdir('models'):\n",
    "    os.makedirs('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c0c2a-a708-4de7-a8fd-59c474e514ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in tqdm(range(1, EPISODES + 1), ascii=Ture, unit='episodes'):\n",
    "    agent.tensorboard.step = episode\n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "    current_state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.argmax(agent.get_qs(current_state))\n",
    "        else:\n",
    "            action = np.random.randint(0, env.ACTION_SPACE_SIZE)\n",
    "        \n",
    "        new_state, reward, done = env.step(action)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if SHOW_PREVIEW and not episode % AGGREGATE_STATS_EVERY:\n",
    "            env.render()\n",
    "            \n",
    "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "        agent.train(done, step)\n",
    "        current_state = new_state\n",
    "        step += 1\n",
    "        \n",
    "    ep_rewards.append(episode_reward)\n",
    "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:]) / len(ep_rewards[--AGGREGATE_STATS_EVERY:])\n",
    "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        agent.tensorboard.update_stats(reward_avg=average_reward, \n",
    "                                       reward_min=min_reward,\n",
    "                                       reward_max=max_reward,\n",
    "                                       epsilon=epsilon,\n",
    "                                      )\n",
    "        \n",
    "        if average_reward >= MIN_REWARD:\n",
    "            agent.model.save(f'models/{MODEL_NAME}_{max_reward:_>7.2f}\n",
    "                                max_{average_reward:_>7.2f}\n",
    "                                min_{int(time.time())}.model')\n",
    "            \n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
